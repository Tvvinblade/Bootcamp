from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date
import os


jdbc_url = os.getenv('POSTGRES_JDBC_URL')
db_user = os.getenv('POSTGRES_USER')
db_password = os.getenv('POSTGRES_PASSWORD')
table_name = os.getenv('TABLE_NAME')
s3_path = os.getenv('S3_APP_INSTALLS_PATH')


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
spark = SparkSession.builder \
    .appName("JdbcToS3AppInstalls") \
    .config("spark.ui.port", "4041") \
    .getOrCreate()


# –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ñ–∞–π–ª—ã –≤ S3
try:
    existing_data_df = spark.read.parquet(s3_path)
    max_ts = existing_data_df.selectExpr("MAX(ts) as max_ts").collect()[0]["max_ts"]
    print(f"üîÅ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å ts > {max_ts}")
    predicate = f"ts > timestamp '{max_ts}'"
except Exception as e:
    print("üÜï –î–∞–Ω–Ω—ã—Ö –≤ S3 –Ω–µ—Ç, –∑–∞–≥—Ä—É–∑–∏–º –≤—Å—ë –∏–∑ –±–∞–∑—ã.")
    predicate = "1=1"

# –ß—Ç–µ–Ω–∏–µ –∏–∑ PostgreSQL
jdbc_df = spark.read \
    .format("jdbc") \
    .option("url", jdbc_url) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("dbtable", table_name) \
    .option("fetchsize", 1000) \
    .option("driver", "org.postgresql.Driver") \
    .option("pushDownPredicate", "true") \
    .load() \
    .filter(predicate)

# –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞—Ç–æ–π –¥–ª—è –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
df_with_partition = jdbc_df \
    .withColumn("event_date", to_date(col("ts")))


# –ó–∞–ø–∏—Å—å –≤ S3 —Å –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
df_with_partition.write \
    .mode("append") \
    .partitionBy("event_date") \
    .parquet(s3_path)

print("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
